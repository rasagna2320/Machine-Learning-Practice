{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dfde648b",
      "metadata": {
        "papermill": {
          "duration": 0.002767,
          "end_time": "2025-10-21T19:47:44.778496",
          "exception": false,
          "start_time": "2025-10-21T19:47:44.775729",
          "status": "completed"
        },
        "tags": [],
        "id": "dfde648b"
      },
      "source": [
        "# Q1. Simple Perceptorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e62df580",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T19:47:44.786058Z",
          "iopub.status.busy": "2025-10-21T19:47:44.785164Z",
          "iopub.status.idle": "2025-10-21T19:47:44.792523Z",
          "shell.execute_reply": "2025-10-21T19:47:44.791835Z"
        },
        "papermill": {
          "duration": 0.012426,
          "end_time": "2025-10-21T19:47:44.793773",
          "exception": false,
          "start_time": "2025-10-21T19:47:44.781347",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "e62df580",
        "outputId": "45eab950-ac19-4dee-92a7-b1f2e7293b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9990889488055994\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "\n",
        "# Our activation function: f(x) = 1 / (1 + e^(-x))\n",
        "\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Neuron:\n",
        "\n",
        "    def __init__(self, weights, bias):\n",
        "\n",
        "        self.weights = weights\n",
        "\n",
        "        self.bias = bias\n",
        "\n",
        "    def feedforward(self, inputs):\n",
        "\n",
        "# Weight inputs, add bias, then use the activation function\n",
        "\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "\n",
        "        return sigmoid(total)\n",
        "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
        "\n",
        "bias = 4 # b = 4\n",
        "\n",
        "n = Neuron(weights, bias)\n",
        "\n",
        "x = np.array([2, 3]) # x1 = 2, x2 = 3\n",
        "\n",
        "print(n.feedforward(x)) # 0.9990889488055994"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390a432a",
      "metadata": {
        "papermill": {
          "duration": 0.002701,
          "end_time": "2025-10-21T19:47:44.800434",
          "exception": false,
          "start_time": "2025-10-21T19:47:44.797733",
          "status": "completed"
        },
        "tags": [],
        "id": "390a432a"
      },
      "source": [
        "# Q2. Perceptron with activation Function with AND, OR (linear data), XOR (non linear data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "da746068",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T19:47:44.807188Z",
          "iopub.status.busy": "2025-10-21T19:47:44.806896Z",
          "iopub.status.idle": "2025-10-21T19:47:44.822798Z",
          "shell.execute_reply": "2025-10-21T19:47:44.821875Z"
        },
        "papermill": {
          "duration": 0.021012,
          "end_time": "2025-10-21T19:47:44.824246",
          "exception": false,
          "start_time": "2025-10-21T19:47:44.803234",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "da746068",
        "outputId": "e0553baf-ca2a-4c7a-e0c4-f5e26b80551f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, epochs=100):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def step_function(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(n_samples):\n",
        "                linear_output = np.dot(X[i], self.weights) + self.bias\n",
        "                y_pred = self.step_function(linear_output)\n",
        "                update = self.learning_rate * (y[i] - y_pred)\n",
        "                self.weights += update * X[i]\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = self.step_function(linear_output)\n",
        "        return y_pred\n",
        "\n",
        "# AND gate example\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "perceptron = Perceptron(learning_rate=0.1, epochs=10)\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "test_data = np.array([[1, 1], [0, 1]])\n",
        "predictions = perceptron.predict(test_data)\n",
        "print(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4894c2d4",
      "metadata": {
        "papermill": {
          "duration": 0.002806,
          "end_time": "2025-10-21T19:47:44.830271",
          "exception": false,
          "start_time": "2025-10-21T19:47:44.827465",
          "status": "completed"
        },
        "tags": [],
        "id": "4894c2d4"
      },
      "source": [
        "# Q3. MLP with single hidden Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "31f49185",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T19:47:44.837693Z",
          "iopub.status.busy": "2025-10-21T19:47:44.836939Z",
          "iopub.status.idle": "2025-10-21T19:47:44.846531Z",
          "shell.execute_reply": "2025-10-21T19:47:44.845835Z"
        },
        "papermill": {
          "duration": 0.014732,
          "end_time": "2025-10-21T19:47:44.847901",
          "exception": false,
          "start_time": "2025-10-21T19:47:44.833169",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "31f49185",
        "outputId": "b4580e9f-f888-45a7-9fcb-a6dda044d127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7216325609518421\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# --- Activation Function ---\n",
        "\n",
        "def sigmoid(x):\n",
        "\n",
        "    \"\"\"Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\"\"\"\n",
        "\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# --- Neuron Class ---\n",
        "\n",
        "class Neuron:\n",
        "\n",
        "    def __init__(self, weights, bias):\n",
        "\n",
        "        self.weights = weights\n",
        "\n",
        "        self.bias = bias\n",
        "\n",
        "    def feedforward(self, inputs):\n",
        "\n",
        "        \"\"\"Compute neuron output using weights, bias, and activation function.\"\"\"\n",
        "\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "\n",
        "        return sigmoid(total)\n",
        "class OurNeuralNetwork:\n",
        "\n",
        "    '''\n",
        "\n",
        "A neural network with:\n",
        "\n",
        "- 2 inputs\n",
        "\n",
        "- a hidden layer with 2 neurons (h1, h2)\n",
        "\n",
        "- an output layer with 1 neuron (o1)\n",
        "\n",
        "Each neuron has the same weights and bias:\n",
        "\n",
        "- w = [0, 1]\n",
        "\n",
        "- b = 0\n",
        "\n",
        "        '''\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        weights = np.array([0, 1])\n",
        "\n",
        "        bias = 0\n",
        "# The Neuron class here is from the previous section\n",
        "\n",
        "        self.h1 = Neuron(weights, bias)\n",
        "\n",
        "        self.h2 = Neuron(weights, bias)\n",
        "\n",
        "        self.o1 = Neuron(weights, bias)\n",
        "\n",
        "    def feedforward(self, x):\n",
        "\n",
        "        out_h1 = self.h1.feedforward(x)\n",
        "\n",
        "        out_h2 = self.h2.feedforward(x)\n",
        "\n",
        "# The inputs for o1 are the outputs from h1 and h2\n",
        "\n",
        "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
        "\n",
        "        return out_o1\n",
        "\n",
        "network = OurNeuralNetwork()\n",
        "\n",
        "x = np.array([2, 3])\n",
        "\n",
        "print(network.feedforward(x)) # 0.7216325609518421"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3adb29c5",
      "metadata": {
        "papermill": {
          "duration": 0.002872,
          "end_time": "2025-10-21T19:47:44.853852",
          "exception": false,
          "start_time": "2025-10-21T19:47:44.850980",
          "status": "completed"
        },
        "tags": [],
        "id": "3adb29c5"
      },
      "source": [
        "# Q4. MLP on MNIST dataset and display its train and test data score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "afceca3c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T19:47:44.861416Z",
          "iopub.status.busy": "2025-10-21T19:47:44.860650Z",
          "iopub.status.idle": "2025-10-21T19:49:57.541173Z",
          "shell.execute_reply": "2025-10-21T19:49:57.539590Z"
        },
        "papermill": {
          "duration": 132.687278,
          "end_time": "2025-10-21T19:49:57.543993",
          "exception": true,
          "start_time": "2025-10-21T19:47:44.856715",
          "status": "failed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "afceca3c",
        "outputId": "f8985e62-61e5-4ca3-907f-4d649dbc3c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (70000, 784)\n",
            "Labels shape: (70000,)\n",
            "X_train shape: (63000, 784)\n",
            "X_test shape: (7000, 784)\n",
            "Iteration 1, loss = 0.36947524\n",
            "Iteration 2, loss = 0.16403673\n",
            "Iteration 3, loss = 0.11692557\n",
            "Iteration 4, loss = 0.09073399\n",
            "Iteration 5, loss = 0.07360339\n",
            "Iteration 6, loss = 0.06079920\n",
            "Iteration 7, loss = 0.05128877\n",
            "Iteration 8, loss = 0.04315299\n",
            "Iteration 9, loss = 0.03573000\n",
            "Iteration 10, loss = 0.03089252\n",
            "Iteration 11, loss = 0.02634015\n",
            "Iteration 12, loss = 0.02260335\n",
            "Iteration 13, loss = 0.01888108\n",
            "Iteration 14, loss = 0.01641133\n",
            "Iteration 15, loss = 0.01391740\n",
            "Iteration 16, loss = 0.01179918\n",
            "Iteration 17, loss = 0.00961449\n",
            "Iteration 18, loss = 0.00847527\n",
            "Iteration 19, loss = 0.00719428\n",
            "Iteration 20, loss = 0.00637856\n",
            "Iteration 21, loss = 0.00533186\n",
            "Iteration 22, loss = 0.00474905\n",
            "Iteration 23, loss = 0.00577574\n",
            "Iteration 24, loss = 0.00586461\n",
            "Iteration 25, loss = 0.00325895\n",
            "Iteration 26, loss = 0.00273516\n",
            "Iteration 27, loss = 0.00203375\n",
            "Iteration 28, loss = 0.00176265\n",
            "Iteration 29, loss = 0.00163496\n",
            "Iteration 30, loss = 0.00144681\n",
            "Iteration 31, loss = 0.00620124\n",
            "Iteration 32, loss = 0.01144387\n",
            "Iteration 33, loss = 0.00267516\n",
            "Iteration 34, loss = 0.00150739\n",
            "Iteration 35, loss = 0.00114585\n",
            "Iteration 36, loss = 0.00107412\n",
            "Iteration 37, loss = 0.00102356\n",
            "Iteration 38, loss = 0.00097537\n",
            "Iteration 39, loss = 0.00095127\n",
            "Iteration 40, loss = 0.00092117\n",
            "Iteration 41, loss = 0.00089527\n",
            "Iteration 42, loss = 0.00086589\n",
            "Iteration 43, loss = 0.00088237\n",
            "Iteration 44, loss = 0.00107800\n",
            "Iteration 45, loss = 0.01933492\n",
            "Iteration 46, loss = 0.00250233\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Training set accuracy: 0.999984126984127\n",
            "Testing set accuracy: 0.98\n",
            "Predicted value: 9\n",
            "Actual value: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets as skl_data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# --- 1. Load MNIST dataset ---\n",
        "data, labels = skl_data.fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "\n",
        "# Convert labels from string to integer\n",
        "labels = labels.astype(int)\n",
        "\n",
        "# Normalize pixel values to [0,1]\n",
        "data = data / 255.0\n",
        "\n",
        "print(\"Data shape:\", data.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "# --- 2. Train-test split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, labels, test_size=0.10, random_state=42, stratify=labels\n",
        ")\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "\n",
        "# --- 3. Create MLP classifier ---\n",
        "# Single hidden layer with 175 neurons\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(175,),\n",
        "    max_iter=200,\n",
        "    verbose=1,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "# --- 4. Train MLP ---\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# --- 5. Evaluate ---\n",
        "print(\"Training set accuracy:\", mlp.score(X_train, y_train))\n",
        "print(\"Testing set accuracy:\", mlp.score(X_test, y_test))\n",
        "\n",
        "# --- 6. Predict a single digit ---\n",
        "index = 346\n",
        "test_digit = X_test.iloc[index].to_numpy().reshape(1, 784)  # use .iloc for row\n",
        "test_digit_prediction = mlp.predict(test_digit)[0]\n",
        "\n",
        "print(\"Predicted value:\", test_digit_prediction)\n",
        "print(\"Actual value:\", y_test.iloc[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f32a04d",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "0f32a04d"
      },
      "source": [
        "# Q5. Find the relation between learning rate and loss (or) number  of iterations and loss."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
        "\n",
        "# Test relation: Learning Rate vs Loss\n",
        "for lr in [0.1, 0.01, 0.001]:\n",
        "    print(f\"\\n➡ Learning Rate = {lr}\")\n",
        "\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=lr),\n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(x_train, y_train, epochs=5, batch_size=64, verbose=0)\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    print(f\"Final Training Loss = {final_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FgrqGqD-FczB",
        "outputId": "e1b94ed8-10b6-4397-a132-773917f4830a"
      },
      "id": "FgrqGqD-FczB",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "➡ Learning Rate = 0.1\n",
            "Final Training Loss = 1.0227\n",
            "\n",
            "➡ Learning Rate = 0.01\n",
            "Final Training Loss = 0.0953\n",
            "\n",
            "➡ Learning Rate = 0.001\n",
            "Final Training Loss = 0.0565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "562ab6ba",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "562ab6ba"
      },
      "source": [
        "#  Q6. Find the relation between number of neurons with learning rate and iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5fca9c2e",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-10-21T19:47:17.385309Z",
          "iopub.status.idle": "2025-10-21T19:47:17.385746Z",
          "shell.execute_reply": "2025-10-21T19:47:17.385548Z",
          "shell.execute_reply.started": "2025-10-21T19:47:17.385530Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "5fca9c2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9473ecc0-4cd4-4b41-9a64-419c68262661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "Neurons=32, Learning Rate=0.01, Epochs=3\n",
            "Test Accuracy=0.9569, Test Loss=0.1503\n",
            "\n",
            "Neurons=32, Learning Rate=0.01, Epochs=5\n",
            "Test Accuracy=0.9549, Test Loss=0.1619\n",
            "\n",
            "Neurons=32, Learning Rate=0.001, Epochs=3\n",
            "Test Accuracy=0.9527, Test Loss=0.1588\n",
            "\n",
            "Neurons=32, Learning Rate=0.001, Epochs=5\n",
            "Test Accuracy=0.9603, Test Loss=0.1324\n",
            "\n",
            "Neurons=64, Learning Rate=0.01, Epochs=3\n",
            "Test Accuracy=0.9558, Test Loss=0.1523\n",
            "\n",
            "Neurons=64, Learning Rate=0.01, Epochs=5\n",
            "Test Accuracy=0.9582, Test Loss=0.1559\n",
            "\n",
            "Neurons=64, Learning Rate=0.001, Epochs=3\n",
            "Test Accuracy=0.9611, Test Loss=0.1292\n",
            "\n",
            "Neurons=64, Learning Rate=0.001, Epochs=5\n",
            "Test Accuracy=0.9691, Test Loss=0.0992\n",
            "\n",
            "Neurons=128, Learning Rate=0.01, Epochs=3\n",
            "Test Accuracy=0.9683, Test Loss=0.1306\n",
            "\n",
            "Neurons=128, Learning Rate=0.01, Epochs=5\n",
            "Test Accuracy=0.9678, Test Loss=0.1509\n",
            "\n",
            "Neurons=128, Learning Rate=0.001, Epochs=3\n",
            "Test Accuracy=0.9719, Test Loss=0.0918\n",
            "\n",
            "Neurons=128, Learning Rate=0.001, Epochs=5\n",
            "Test Accuracy=0.9738, Test Loss=0.0792\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Test different Neurons, Learning Rates, and Epochs\n",
        "for neurons in [32, 64, 128]:\n",
        "    for lr in [0.01, 0.001]:\n",
        "        for epochs in [3, 5]:\n",
        "            print(f\"\\nNeurons={neurons}, Learning Rate={lr}, Epochs={epochs}\")\n",
        "\n",
        "            model = Sequential([\n",
        "                Flatten(input_shape=(28, 28)),\n",
        "                Dense(neurons, activation='relu'),\n",
        "                Dense(10, activation='softmax')\n",
        "            ])\n",
        "            model.compile(optimizer=Adam(learning_rate=lr),\n",
        "                          loss='categorical_crossentropy',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "            model.fit(x_train, y_train, epochs=epochs, batch_size=64, verbose=0)\n",
        "            loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "            print(f\"Test Accuracy={acc:.4f}, Test Loss={loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31153,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 139.610948,
      "end_time": "2025-10-21T19:49:58.170666",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-10-21T19:47:38.559718",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}